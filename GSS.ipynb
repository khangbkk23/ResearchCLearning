{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169a79a6",
   "metadata": {},
   "source": [
    "# GSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bee78878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Notebook last modified at: 2025-07-28 23:25:44\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchvision.models import resnet34 as torchvision_resnet34\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, random_split, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Checking status of GPU and time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Notebook last modified at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a8a15",
   "metadata": {},
   "source": [
    "## ReplayBuffer for EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_per_class=100, model=None, loss_fn=None, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer with GSS-Greedy selection.\n",
    "        \n",
    "        Args:\n",
    "            max_per_class (int): Maximum number of exemplars per class.\n",
    "            model: Neural network model for gradient computation (e.g., ResNetSmall).\n",
    "            loss_fn: Loss function for gradient computation (e.g., CrossEntropyLoss).\n",
    "            device: Device to perform computations (cuda or cpu).\n",
    "        \"\"\"\n",
    "        self.max_per_class = max_per_class\n",
    "        self.buffer = defaultdict(list)\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn or nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        self.device = device\n",
    "\n",
    "    def compute_gradient(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute normalized gradient for a single sample.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (single sample).\n",
    "            y (torch.Tensor): Label tensor (single label).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Normalized gradient vector.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        x, y = x.unsqueeze(0).to(self.device), y.unsqueeze(0).to(self.device)\n",
    "        self.model.zero_grad()\n",
    "        feats, logits = self.model(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Collect gradients\n",
    "        grad = []\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad.append(param.grad.flatten())\n",
    "        grad = torch.cat(grad)\n",
    "        grad_norm = torch.norm(grad, p=2)\n",
    "        return grad / (grad_norm + 1e-8)  # Normalize gradient\n",
    "\n",
    "    def add_examples(self, x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Add examples to the replay buffer using GSS-Greedy selection.\n",
    "        \n",
    "        Args:\n",
    "            x_batch (torch.Tensor): Batch of input data.\n",
    "            y_batch (torch.Tensor): Corresponding labels for the input data.\n",
    "        \"\"\"\n",
    "        if self.model is None or self.loss_fn is None:\n",
    "            raise ValueError(\"Model and loss function must be set for GSS-Greedy selection.\")\n",
    "        \n",
    "        for cls in set(y_batch.cpu().numpy()):\n",
    "            cls = int(cls)\n",
    "            # Collect samples for this class\n",
    "            cls_indices = (y_batch == cls).nonzero(as_tuple=True)[0]\n",
    "            if not cls_indices.numel():\n",
    "                continue\n",
    "            \n",
    "            cls_samples = [(x_batch[i], y_batch[i]) for i in cls_indices]\n",
    "            current_samples = self.buffer[cls] + cls_samples\n",
    "            \n",
    "            if len(current_samples) <= self.max_per_class:\n",
    "                # If within limit, keep all samples\n",
    "                self.buffer[cls] = current_samples\n",
    "            else:\n",
    "                # Compute gradients for all samples\n",
    "                gradients = []\n",
    "                samples = []\n",
    "                for x, y in current_samples:\n",
    "                    grad = self.compute_gradient(x, y)\n",
    "                    gradients.append(grad)\n",
    "                    samples.append((x, y))\n",
    "                \n",
    "                # GSS-Greedy selection\n",
    "                selected_indices = []\n",
    "                for _ in range(self.max_per_class):\n",
    "                    if not selected_indices:\n",
    "                        idx = np.random.randint(0, len(samples))\n",
    "                    else:\n",
    "                        distances = []\n",
    "                        for i, grad in enumerate(gradients):\n",
    "                            if i in selected_indices:\n",
    "                                continue\n",
    "                            min_dist = min([torch.norm(grad - gradients[j]) for j in selected_indices])\n",
    "                            distances.append((i, min_dist))\n",
    "                        idx = max(distances, key=lambda x: x[1])[0]\n",
    "                    selected_indices.append(idx)\n",
    "                \n",
    "                # Update buffer with selected samples\n",
    "                self.buffer[cls] = [samples[i] for i in selected_indices]\n",
    "\n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        Get all data from the replay buffer as a TensorDataset.\n",
    "        \n",
    "        Returns:\n",
    "            TensorDataset: A dataset containing all exemplars in the buffer.\n",
    "        \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for cls, examples in self.buffer.items():\n",
    "            if examples:\n",
    "                xs.extend([x for x, _ in examples])\n",
    "                ys.extend([y for _, y in examples])\n",
    "        if not xs:\n",
    "            return None, None\n",
    "        return torch.stack(xs), torch.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 3. Elastic Weight Consolidation (EWC) ====  \n",
    "class EWC:\n",
    "    def __init__(self, model, dataloader, device, samples=500):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.params = {n: p.clone().detach() for n, p in model.named_parameters()}\n",
    "        self.fisher = self._compute_fisher(dataloader, samples)\n",
    "\n",
    "    def _compute_fisher(self, dataloader, samples):\n",
    "        fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters()}\n",
    "        self.model.eval()\n",
    "        count = 0\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(self.device)\n",
    "            self.model.zero_grad()\n",
    "            feats, logits = self.model(x)\n",
    "            # prob = F.softmax(logits, dim=1)\n",
    "\n",
    "            # log_prob = F.log_softmax(logits, dim=1)[range(len(y)), y].mean()\n",
    "            # log_prob.backward()\n",
    "\n",
    "            log_probs = F.log_softmax(logits, dim=1)\n",
    "            # sum negative log‐likelihood over batch\n",
    "            loss_batch = -log_probs[range(len(y)), y].sum()\n",
    "            loss_batch.backward()\n",
    "            for n, p in self.model.named_parameters():\n",
    "                fisher[n] += p.grad.data.pow(2)\n",
    "            count += x.size(0)   # count by number of *samples*\n",
    "            if count >= samples:\n",
    "                break\n",
    "        return {n: f / count for n, f in fisher.items()}\n",
    "\n",
    "    def penalty(self, model, lambda_ewc):\n",
    "        loss = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            if n not in self.fisher: continue\n",
    "            f, p0 = self.fisher[n], self.params[n]\n",
    "            if p.shape == p0.shape:\n",
    "                loss += (f * (p - p0).pow(2)).sum()\n",
    "            else:\n",
    "                # assume this is the expanded fc.weight or fc.bias\n",
    "                # only penalize the first p0.shape[...] entries\n",
    "                if 'fc.weight' in n and p.dim()==2:\n",
    "                    loss += (f * (p[:p0.size(0)] - p0).pow(2)).sum()\n",
    "                elif 'fc.bias' in n and p.dim()==1:\n",
    "                    loss += (f * (p[:p0.size(0)] - p0).pow(2)).sum()\n",
    "        return (lambda_ewc / 2) * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5963ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerTaskEWC:\n",
    "    \"\"\"\n",
    "    Collects multiple (params, fisher) snapshots—one per past task—\n",
    "    and, at training‐time, computes the sum of all EWC penalties.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, device, ewc_paths: list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - model (nn.Module):  The “current” model (whose parameter names\n",
    "                                must match those stored on disk).\n",
    "          - device:            CPU / CUDA device.\n",
    "          - ewc_paths:         List of file‐paths: ['ewc_task_1.pt', 'ewc_task_2.pt', ...].\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "\n",
    "        self.past_task_params = []  # list of dict: each dict maps name→tensor (θ^{*(k)})\n",
    "        self.past_task_fishers = [] # list of dict: each dict maps name→tensor (F^{(k)})\n",
    "\n",
    "        # Load all saved EWC files:\n",
    "        for path in ewc_paths:\n",
    "            data = torch.load(path, map_location='cpu')\n",
    "            # data['params'] and data['fisher'] are both dict(name→cpu_tensor)\n",
    "            # Move them to the correct device now:\n",
    "            params_k = {name: param.to(self.device) for name, param in data['params'].items()}\n",
    "            fisher_k = {name: fisher.to(self.device) for name, fisher in data['fisher'].items()}\n",
    "            self.past_task_params.append(params_k)\n",
    "            self.past_task_fishers.append(fisher_k)\n",
    "\n",
    "\n",
    "    def penalty(self, model, lambda_ewc):\n",
    "        \"\"\"\n",
    "        Loops over each past task k, then each parameter name,\n",
    "        and accumulates F^{(k)}_i * (θ_i - θ^{*(k)}_i)^2.\n",
    "\n",
    "        Returns:  (λ/2) * [sum over tasks & params of F (θ - θ*)^2]\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Iterate over each past‐task snapshot:\n",
    "        for params_k, fisher_k in zip(self.past_task_params, self.past_task_fishers):\n",
    "            for name, param in model.named_parameters():\n",
    "                # If this parameter existed when snapshot_k was taken:\n",
    "                if name not in fisher_k:\n",
    "                    continue\n",
    "\n",
    "                θ_star = params_k[name]      # θ^{*(k)}\n",
    "                Fk      = fisher_k[name]     # F^{(k)}\n",
    "\n",
    "                if param.shape == θ_star.shape:\n",
    "                    total_loss += (Fk * (param - θ_star).pow(2)).sum()\n",
    "                else:\n",
    "                    # If some layers were expanded (e.g. classifier head grew),\n",
    "                    # only penalize the “old” slice [0:θ_star.shape[...]].\n",
    "                    if 'fc.weight' in name and param.dim() == 2:\n",
    "                        total_loss += (Fk * (param[:θ_star.size(0)] - θ_star).pow(2)).sum()\n",
    "                    elif 'fc.bias' in name and param.dim() == 1:\n",
    "                        total_loss += (Fk * (param[:θ_star.size(0)] - θ_star).pow(2)).sum()\n",
    "                    # else: if other layers changed shape unexpectely, you may skip them.\n",
    "\n",
    "        # Multiply by λ/2:\n",
    "        return (lambda_ewc / 2) * total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNetSmall(nn.Module):\n",
    "    \"\"\"\n",
    "    Small ResNet-34 model with 2 blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = BasicBlock(16, 32, stride=2)\n",
    "        self.layer2 = BasicBlock(32, 64, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.feature_dim = 64\n",
    "        self.fc = nn.Linear(self.feature_dim, num_classes)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"\n",
    "            Extract features from the input tensor.\n",
    "            Params:\n",
    "                x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "            Returns:\n",
    "                torch.Tensor: Extracted features of shape (batch_size, feature_dim).\n",
    "        \"\"\"\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.avgpool(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass through the network.\n",
    "            Params:\n",
    "                x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "        \"\"\"\n",
    "        feats = self.extract_features(x)\n",
    "        logits = self.fc(feats)\n",
    "        return feats, logits\n",
    "\n",
    "    def expand_output(self, new_num_classes):\n",
    "        \"\"\"\n",
    "        Expand the output layer to accommodate new classes.\n",
    "        Args:\n",
    "            new_num_classes (int): The new number of classes for the output layer.\n",
    "        \"\"\"\n",
    "        old_fc = self.fc\n",
    "        new_fc = nn.Linear(self.feature_dim, new_num_classes)\n",
    "        with torch.no_grad():\n",
    "            # copy old parameters of FC layer to newly expanded model\n",
    "            new_fc.weight[:old_fc.out_features] = old_fc.weight\n",
    "            new_fc.bias[:old_fc.out_features] = old_fc.bias\n",
    "        self.fc = new_fc.to(old_fc.weight.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3707895",
   "metadata": {},
   "source": [
    "## Implement GSS greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1825b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(model, x, y, loss_fn, device):\n",
    "    \"\"\"Compute normalized gradient for a single sample\"\"\"\n",
    "    model.eval()\n",
    "    x, y = x.unsqueeze(0).to(device), y.unsqueeze(0).to(device)\n",
    "    model.zero_grad()\n",
    "    features, logits = model(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Collect gradients\n",
    "    grad = []\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            grad.append(param.grad.flatten())\n",
    "    grad = torch.cat(grad)\n",
    "    grad_norm = torch.norm(grad, p=2)\n",
    "    return grad / (grad_norm + 1e-8)  # Normalize gradient\n",
    "\n",
    "def gss_greedy_selection(model, data_loader, num_exemplars_per_class, loss_fn, device):\n",
    "    \"\"\"GSS-Greedy exemplar selection\"\"\"\n",
    "    model.eval()\n",
    "    exemplars = defaultdict(list)\n",
    "    for class_id in range(data_loader.dataset.num_classes):\n",
    "        # Get class-specific data\n",
    "        class_samples = [(x, y) for x, y in data_loader.dataset if y == class_id]\n",
    "        if not class_samples:\n",
    "            continue\n",
    "        \n",
    "        # Compute gradients for all samples\n",
    "        gradients = []\n",
    "        samples = []\n",
    "        for x, y in class_samples:\n",
    "            grad = compute_gradient(model, x, y, loss_fn, device)\n",
    "            gradients.append(grad)\n",
    "            samples.append((x, y))\n",
    "        \n",
    "        # Greedy selection\n",
    "        selected_indices = []\n",
    "        for _ in range(min(num_exemplars_per_class, len(samples))):\n",
    "            if not selected_indices:\n",
    "                # Pick first sample randomly\n",
    "                idx = np.random.randint(0, len(samples))\n",
    "            else:\n",
    "                # Compute max-min distance in gradient space\n",
    "                distances = []\n",
    "                for i, grad in enumerate(gradients):\n",
    "                    if i in selected_indices:\n",
    "                        continue\n",
    "                    min_dist = min([torch.norm(grad - gradients[j]) for j in selected_indices])\n",
    "                    distances.append((i, min_dist))\n",
    "                idx = max(distances, key=lambda x: x[1])[0]\n",
    "            selected_indices.append(idx)\n",
    "        \n",
    "        # Store selected exemplars\n",
    "        for idx in selected_indices:\n",
    "            exemplars[class_id].append(samples[idx])\n",
    "    \n",
    "    return exemplars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5fa1ec",
   "metadata": {},
   "source": [
    "## Implement train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c7c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher(model, dataloader, device, samples=500):\n",
    "    \"\"\"\n",
    "    Compute Fisher information for the model’s parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model.\n",
    "        dataloader: DataLoader for the task.\n",
    "        device: Device for computations.\n",
    "        samples: Number of samples to use for Fisher computation.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Fisher information for each parameter.\n",
    "    \"\"\"\n",
    "    fisher = {n: torch.zeros_like(p) for n, p in model.named_parameters()}\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        model.zero_grad()\n",
    "        _, logits = model(x)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = -log_probs[range(len(y)), y].sum()\n",
    "        loss.backward()\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                fisher[n] += p.grad.data.pow(2)\n",
    "        count += x.size(0)\n",
    "        if count >= samples:\n",
    "            break\n",
    "    return {n: f / count for n, f in fisher.items()}\n",
    "\n",
    "def save_ewc_snapshot(model, dataloader, device, path, samples=500):\n",
    "    \"\"\"\n",
    "    Save EWC snapshot (parameters and Fisher information) for a task.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model.\n",
    "        dataloader: DataLoader for the task.\n",
    "        device: Device for computations.\n",
    "        path: File path to save the snapshot.\n",
    "        samples: Number of samples for Fisher computation.\n",
    "    \"\"\"\n",
    "    params = {n: p.clone().detach() for n, p in model.named_parameters()}\n",
    "    fisher = compute_fisher(model, dataloader, device, samples)\n",
    "    torch.save({'params': params, 'fisher': fisher}, path)\n",
    "\n",
    "def train_and_plot(train_loaders, test_loaders, ood_loader, device, args):\n",
    "    \"\"\"\n",
    "    Train the model with GSS-based replay buffer and PerTaskEWC regularization.\n",
    "    \n",
    "    Args:\n",
    "        train_loaders (list): List of DataLoader objects for training tasks.\n",
    "        test_loaders (list): List of DataLoader objects for testing tasks.\n",
    "        ood_loader (DataLoader): DataLoader for out-of-distribution data.\n",
    "        device (torch.device): Device for computations.\n",
    "        args: Hyperparameter arguments.\n",
    "    \"\"\"\n",
    "    # Initialize model and replay buffer\n",
    "    model = ResNetSmall(num_classes=args.num_cls_per_task).to(device)\n",
    "    replay_buffer = ReplayBuffer(max_per_class=args.max_per_class, model=model, device=device)\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    \n",
    "    # Store paths for EWC snapshots\n",
    "    ewc_paths = []\n",
    "    num_classes_seen = 0\n",
    "    \n",
    "    # Training loop over tasks\n",
    "    for task_id, train_loader in enumerate(train_loaders):\n",
    "        print(f\"Training on task {task_id + 1}/{len(train_loaders)}\")\n",
    "        \n",
    "        # Expand output layer for new classes\n",
    "        num_classes_seen += args.num_cls_per_task\n",
    "        model.expand_output(num_classes_seen)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Initialize PerTaskEWC for this task (with all past snapshots)\n",
    "        if ewc_paths:\n",
    "            ewc = PerTaskEWC(model, device, ewc_paths)\n",
    "        else:\n",
    "            ewc = None\n",
    "        \n",
    "        # Train for multiple epochs\n",
    "        for epoch in range(args.epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for x, y in train_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                # Add exemplars from replay buffer\n",
    "                ex_x, ex_y = replay_buffer.get_all_data()\n",
    "                if ex_x is not None:\n",
    "                    x = torch.cat([x, ex_x.to(device)], dim=0)\n",
    "                    y = torch.cat([y, ex_y.to(device)], dim=0)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                feats, logits = model(x)\n",
    "                loss = loss_fn(logits, y)\n",
    "                \n",
    "                # Apply PerTaskEWC penalty\n",
    "                if ewc is not None:\n",
    "                    loss += ewc.penalty(model, args.lambda_ewc)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{args.epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        # Update replay buffer with GSS-Greedy\n",
    "        x_batch = torch.cat([x for x, _ in train_loader.dataset], dim=0)\n",
    "        y_batch = torch.cat([y for _, y in train_loader.dataset], dim=0)\n",
    "        replay_buffer.add_examples(x_batch, y_batch)\n",
    "        \n",
    "        # Save EWC snapshot for this task\n",
    "        ewc_path = f\"{args.savedir}/ewc_task_{task_id + 1}.pt\"\n",
    "        save_ewc_snapshot(model, train_loader, device, ewc_path, samples=500)\n",
    "        ewc_paths.append(ewc_path)\n",
    "        \n",
    "        # Evaluate on all seen tasks\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        for t, test_loader in enumerate(test_loaders[:task_id + 1]):\n",
    "            correct, total = 0, 0\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.no_grad():\n",
    "                    _, logits = model(x)\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    correct += (preds == y).sum().item()\n",
    "                    total += y.size(0)\n",
    "            acc = correct / total\n",
    "            accuracies.append(acc)\n",
    "            print(f\"Task {t + 1} Accuracy: {acc:.4f}\")\n",
    "        avg_acc = sum(accuracies) / len(accuracies) if accuracies else 0\n",
    "        print(f\"Average Accuracy across tasks: {avg_acc:.4f}\")\n",
    "        \n",
    "        # Save model and buffer\n",
    "        torch.save(model.state_dict(), f\"{args.savedir}/model_task_{task_id + 1}.pth\")\n",
    "        torch.save(replay_buffer.buffer, f\"{args.savedir}/buffer_task_{task_id + 1}.pth\")\n",
    "    \n",
    "    return model, accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
