{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0373d5c",
   "metadata": {},
   "source": [
    "# ICaRL: The strategy of taking buffer for EWC in Continual Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ced9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Notebook last modified at: 2025-07-27 22:38:25\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchvision.models import resnet34 as torchvision_resnet34\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, random_split, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Checking status of GPU and time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Notebook last modified at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ab334",
   "metadata": {},
   "source": [
    "## 1. Replay Buffer with per-class exemplars capturing the distribution of each class\n",
    "- The buffer is used to store a fixed number of exemplars for each class.\n",
    "- The number of exemplars per class is determined by the `exemplars_per_class`\n",
    "  parameter.\n",
    "- The buffer is updated with new exemplars when the model encounters new classes.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17340147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_per_class=100):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer with a maximum number of exemplars per class.\n",
    "        \n",
    "        Args:\n",
    "            max_per_class (int): Maximum number of exemplars to store for each class.\n",
    "        \"\"\"\n",
    "        self.max_per_class = max_per_class\n",
    "        self.buffer = defaultdict(list)\n",
    "        \n",
    "    def add_examples(self, x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Add examples to the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            x_batch (torch.Tensor): Batch of input data.\n",
    "            y_batch (torch.Tensor): Corresponding labels for the input data.\n",
    "        \"\"\"\n",
    "        for x, y in zip(x_batch, y_batch):\n",
    "            cls = int(y.item())\n",
    "            lst = self.buffer[cls]\n",
    "            lst.append(x)\n",
    "            # FIFO replacement if the class buffer exceeds the limit\n",
    "            if len(lst) > self.max_per_class:\n",
    "                lst.pop(0)\n",
    "            self.buffer[cls] = lst\n",
    "    \n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        Get all data from the replay buffer as a TensorDataset.\n",
    "        \n",
    "        Returns:\n",
    "            TensorDataset: A dataset containing all exemplars in the buffer.\n",
    "        \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for cls, examples in self.buffer.item():\n",
    "            xs.append(torch.stack(examples)) # Collect all examples for the class\n",
    "            ys.append(torch.full((len(examples), 1), cls, dtype=torch.long))\n",
    "        if not xs:\n",
    "            return None, None\n",
    "        return torch.cat(xs, dim=0), torch.cat(ys, dim=0)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd6b51",
   "metadata": {},
   "source": [
    "## 2. iCaRL buffer with memory budget memory_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b33891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICaRLBuffer:\n",
    "    def __init__(self, memory_size):\n",
    "        \"\"\"\n",
    "        Initialize the iCaRL buffer with a memory budget.\n",
    "        \n",
    "        Args:\n",
    "            memory_size (int): Total memory budget for the buffer.\n",
    "        \"\"\"\n",
    "        self.memory_size = memory_size # maximum buffer size\n",
    "        self.exemplar_set = defaultdict(list) # class_id -> list of exemplars tensors\n",
    "        self.seen_classes = set() # keep track of seen classes\n",
    "    \n",
    "    def construct_exemplar_set(self, class_id, features, images, m):\n",
    "        \"\"\"\n",
    "        Construct the exemplar set for a given class.\n",
    "        \n",
    "        Args:\n",
    "            class_id (int): The class ID for which to construct the exemplar set.\n",
    "            features (torch.Tensor): Features of the images.\n",
    "            images (torch.Tensor): Corresponding images.\n",
    "            m (int): Number of exemplars to select for this class.\n",
    "        \"\"\"\n",
    "        features = F.normalize(features, dim=1) # normalize features\n",
    "        class_mean = F.normalize(class_mean.unsqueeze(0), dim=1) # normalize class mean\n",
    "        \n",
    "        selected, exemplar_features = [], []\n",
    "        used_indices = torch.zeros(features.size(0), dtype=torch.bool, device=features.device)\n",
    "        \n",
    "        for k in range(m):\n",
    "            if k == 0:\n",
    "                current_sum = 0\n",
    "            else:\n",
    "                current_sum = torch.stack(exemplar_features).sum(dim=0)\n",
    "            mu = class_mean.squeeze(0)\n",
    "            residual = mu * (k + 1) - current_sum\n",
    "            distances = (features @ residual).squeeze()\n",
    "            \n",
    "            # Mask already used indices\n",
    "            distances[used_indices] = float('-inf')\n",
    "            idx = torch.argmax(distances).item()\n",
    "            \n",
    "            selected.append(images[idx].cpu())\n",
    "            exemplar_features.append(features[idx].cpu())\n",
    "            used_indices[idx] = True\n",
    "            \n",
    "        self.exemplar_set[class_id] = selected\n",
    "        self.seen_classes.add(class_id)\n",
    "    \n",
    "    def reduce_exemplar_sets(self, m_per_class):\n",
    "        \"\"\"\n",
    "        Reduce the exemplar sets to fit within the memory budget.\n",
    "        \n",
    "        Args:\n",
    "            m_per_class (int): Number of exemplars to keep per class.\n",
    "        \"\"\"\n",
    "        for cls in self.seen_classes:\n",
    "            if len(self.exemplar_set[cls]) > m_per_class:\n",
    "                self.exemplar_set[cls] = self.exemplar_set[cls][:m_per_class]\n",
    "    \n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        Get all data from the iCaRL buffer as a TensorDataset.\n",
    "        \n",
    "        Returns:\n",
    "            TensorDataset: A dataset containing all exemplars in the buffer.\n",
    "        \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for cls, examples in self.exemplar_set.items():\n",
    "            if examples:\n",
    "                xs.append(torch.stack(examples))\n",
    "                ys.append(torch.full((len(examples), 1), cls, dtype=torch.long))\n",
    "        if not xs:\n",
    "            return None, None\n",
    "        return torch.cat(xs, dim=0), torch.cat(ys, dim=0)\n",
    "    \n",
    "    def get_all_data_for_task(self, class_list):\n",
    "        \"\"\"\n",
    "        Get all data for a specific task from the iCaRL buffer.\n",
    "        \n",
    "        Args:\n",
    "            class_list (list): List of class IDs for the task.\n",
    "        \n",
    "        Returns:\n",
    "            TensorDataset: A dataset containing all exemplars for the specified classes.\n",
    "        \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for cls in class_list:\n",
    "            exemplars = self.exemplar_sets.get(cls, [])\n",
    "            if len(exemplars) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Stack the list-of‐tensors → a single tensor of shape [num_exemplars_of_cls, ...]\n",
    "            xs.append(torch.stack(exemplars))\n",
    "            # Create a label‐tensor of shape [num_exemplars_of_cls] filled with “cls”\n",
    "            ys.append(torch.full((len(exemplars),), cls, dtype=torch.long))\n",
    "\n",
    "        if not xs:\n",
    "            return None, None\n",
    "\n",
    "        # Concatenate along the “batch” dimension\n",
    "        all_x = torch.cat(xs, dim=0)\n",
    "        all_y = torch.cat(ys, dim=0)\n",
    "        return all_x, all_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0255b",
   "metadata": {},
   "source": [
    "## 3. Small ResNet-34 model with 2 blocks\n",
    "- The model is a small ResNet-34 architecture with 2 blocks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
