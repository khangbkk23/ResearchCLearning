{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0373d5c",
   "metadata": {},
   "source": [
    "# ICaRL: The strategy of taking buffer for EWC in Continual Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e37ced9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Notebook last modified at: 2025-07-27 23:02:52\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchvision.models import resnet34 as torchvision_resnet34\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, random_split, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Checking status of GPU and time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Notebook last modified at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ab334",
   "metadata": {},
   "source": [
    "## 1. Replay Buffer with per-class exemplars capturing the distribution of each class\n",
    "- The buffer is used to store a fixed number of exemplars for each class.\n",
    "- The number of exemplars per class is determined by the `exemplars_per_class`\n",
    "  parameter.\n",
    "- The buffer is updated with new exemplars when the model encounters new classes.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17340147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_per_class=100):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer with a maximum number of exemplars per class.\n",
    "        \n",
    "        Args:\n",
    "            max_per_class (int): Maximum number of exemplars to store for each class.\n",
    "        \"\"\"\n",
    "        self.max_per_class = max_per_class\n",
    "        self.buffer = defaultdict(list)\n",
    "        \n",
    "    def add_examples(self, x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Add examples to the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            x_batch (torch.Tensor): Batch of input data.\n",
    "            y_batch (torch.Tensor): Corresponding labels for the input data.\n",
    "        \"\"\"\n",
    "        for x, y in zip(x_batch, y_batch):\n",
    "            cls = int(y.item())\n",
    "            lst = self.buffer[cls]\n",
    "            lst.append(x)\n",
    "            # FIFO replacement if the class buffer exceeds the limit\n",
    "            if len(lst) > self.max_per_class:\n",
    "                lst.pop(0)\n",
    "            self.buffer[cls] = lst\n",
    "    \n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        Get all data from the replay buffer as a TensorDataset.\n",
    "        \n",
    "        Returns:\n",
    "            TensorDataset: A dataset containing all exemplars in the buffer.\n",
    "        \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for cls, examples in self.buffer.item():\n",
    "            xs.append(torch.stack(examples)) # Collect all examples for the class\n",
    "            ys.append(torch.full((len(examples), 1), cls, dtype=torch.long))\n",
    "        if not xs:\n",
    "            return None, None\n",
    "        return torch.cat(xs, dim=0), torch.cat(ys, dim=0)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd6b51",
   "metadata": {},
   "source": [
    "## 2. iCaRL buffer with memory budget memory_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b33891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICaRLBuffer:\n",
    "    def __init__(self, memory_size):\n",
    "        \"\"\"\n",
    "        Initialize the iCaRL buffer with a memory budget.\n",
    "        \n",
    "        Args:\n",
    "            memory_size (int): Total memory budget for the buffer.\n",
    "        \"\"\"\n",
    "        self.memory_size = memory_size # maximum buffer size\n",
    "        self.exemplar_set = defaultdict(list) # class_id -> list of exemplars tensors\n",
    "        self.seen_classes = set() # keep track of seen classes\n",
    "    \n",
    "    def construct_exemplar_set(self, class_id, features, images, m):\n",
    "        \"\"\"\n",
    "        Construct the exemplar set for a given class.\n",
    "        \n",
    "        Args:\n",
    "            class_id (int): The class ID for which to construct the exemplar set.\n",
    "            features (torch.Tensor): Features of the images.\n",
    "            images (torch.Tensor): Corresponding images.\n",
    "            m (int): Number of exemplars to select for this class.\n",
    "        \"\"\"\n",
    "        features = F.normalize(features, dim=1) # normalize features\n",
    "        class_mean = F.normalize(class_mean.unsqueeze(0), dim=1) # normalize class mean\n",
    "        \n",
    "        selected, exemplar_features = [], []\n",
    "        used_indices = torch.zeros(features.size(0), dtype=torch.bool, device=features.device)\n",
    "        \n",
    "        for k in range(m):\n",
    "            if k == 0:\n",
    "                current_sum = 0\n",
    "            else:\n",
    "                current_sum = torch.stack(exemplar_features).sum(dim=0)\n",
    "            mu = class_mean.squeeze(0)\n",
    "            residual = mu * (k + 1) - current_sum\n",
    "            distances = (features @ residual).squeeze()\n",
    "            \n",
    "            # Mask already used indices\n",
    "            distances[used_indices] = float('-inf')\n",
    "            idx = torch.argmax(distances).item()\n",
    "            \n",
    "            selected.append(images[idx].cpu())\n",
    "            exemplar_features.append(features[idx].cpu())\n",
    "            used_indices[idx] = True\n",
    "            \n",
    "        self.exemplar_set[class_id] = selected\n",
    "        self.seen_classes.add(class_id)\n",
    "    \n",
    "    def reduce_exemplar_sets(self, m_per_class):\n",
    "        \"\"\"\n",
    "        Reduce the exemplar sets to fit within the memory budget.\n",
    "        \n",
    "        Args:\n",
    "            m_per_class (int): Number of exemplars to keep per class.\n",
    "        \"\"\"\n",
    "        for cls in self.seen_classes:\n",
    "            if len(self.exemplar_set[cls]) > m_per_class:\n",
    "                self.exemplar_set[cls] = self.exemplar_set[cls][:m_per_class]\n",
    "    \n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        Get all data from the iCaRL buffer as a TensorDataset.\n",
    "        \n",
    "        Returns:\n",
    "            TensorDataset: A dataset containing all exemplars in the buffer.\n",
    "        \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for cls, examples in self.exemplar_set.items():\n",
    "            if examples:\n",
    "                xs.append(torch.stack(examples))\n",
    "                ys.append(torch.full((len(examples), 1), cls, dtype=torch.long))\n",
    "        if not xs:\n",
    "            return None, None\n",
    "        return torch.cat(xs, dim=0), torch.cat(ys, dim=0)\n",
    "    \n",
    "    def get_all_data_for_task(self, class_list):\n",
    "        \"\"\"\n",
    "        Get all data for a specific task from the iCaRL buffer.\n",
    "        \n",
    "        Args:\n",
    "            class_list (list): List of class IDs for the task.\n",
    "        \n",
    "        Returns:\n",
    "            TensorDataset: A dataset containing all exemplars for the specified classes.\n",
    "        \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for cls in class_list:\n",
    "            exemplars = self.exemplar_sets.get(cls, [])\n",
    "            if len(exemplars) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Stack the list-of‐tensors → a single tensor of shape [num_exemplars_of_cls, ...]\n",
    "            xs.append(torch.stack(exemplars))\n",
    "            # Create a label‐tensor of shape [num_exemplars_of_cls] filled with “cls”\n",
    "            ys.append(torch.full((len(exemplars),), cls, dtype=torch.long))\n",
    "\n",
    "        if not xs:\n",
    "            return None, None\n",
    "\n",
    "        # Concatenate along the “batch” dimension\n",
    "        all_x = torch.cat(xs, dim=0)\n",
    "        all_y = torch.cat(ys, dim=0)\n",
    "        return all_x, all_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0255b",
   "metadata": {},
   "source": [
    "## 3. Small ResNet-34 model with 2 blocks\n",
    "- The model is a small ResNet-34 architecture with 2 blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf8cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNetSmall(nn.Module):\n",
    "    \"\"\"\n",
    "    Small ResNet-34 model with 2 blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = BasicBlock(16, 32, stride=2)\n",
    "        self.layer2 = BasicBlock(32, 64, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.feature_dim = 64\n",
    "        self.fc = nn.Linear(self.feature_dim, num_classes)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"\n",
    "            Extract features from the input tensor.\n",
    "            Params:\n",
    "                x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "            Returns:\n",
    "                torch.Tensor: Extracted features of shape (batch_size, feature_dim).\n",
    "        \"\"\"\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.avgpool(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass through the network.\n",
    "            Params:\n",
    "                x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "        \"\"\"\n",
    "        feats = self.extract_features(x)\n",
    "        logits = self.fc(feats)\n",
    "        return feats, logits\n",
    "\n",
    "    def expand_output(self, new_num_classes):\n",
    "        \"\"\"\n",
    "        Expand the output layer to accommodate new classes.\n",
    "        Args:\n",
    "            new_num_classes (int): The new number of classes for the output layer.\n",
    "        \"\"\"\n",
    "        old_fc = self.fc\n",
    "        new_fc = nn.Linear(self.feature_dim, new_num_classes)\n",
    "        with torch.no_grad():\n",
    "            # copy old parameters of FC layer to newly expanded model\n",
    "            new_fc.weight[:old_fc.out_features] = old_fc.weight\n",
    "            new_fc.bias[:old_fc.out_features] = old_fc.bias\n",
    "        self.fc = new_fc.to(old_fc.weight.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b811c",
   "metadata": {},
   "source": [
    "## 4. ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81df3f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1  # For BasicBlock, output channels = out_channels * 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 3)\n",
    "        self.layer2 = self._make_layer(128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 3, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.feature_dim = 512\n",
    "        self.fc = nn.Linear(self.feature_dim, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.extract_features(x)\n",
    "        logits = self.fc(feats)\n",
    "        return feats, logits\n",
    "\n",
    "    def expand_output(self, new_num_classes):\n",
    "        old_fc = self.fc\n",
    "        new_fc = nn.Linear(self.feature_dim, new_num_classes)\n",
    "        with torch.no_grad():\n",
    "            new_fc.weight[:old_fc.out_features] = old_fc.weight\n",
    "            new_fc.bias[:old_fc.out_features] = old_fc.bias\n",
    "        self.fc = new_fc.to(old_fc.weight.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b301b2",
   "metadata": {},
   "source": [
    "## 5. Elastic Weight Consolidation (EWC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcfed9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 3. Elastic Weight Consolidation (EWC) ====  \n",
    "class EWC:\n",
    "    def __init__(self, model, dataloader, device, samples=500):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.params = {n: p.clone().detach() for n, p in model.named_parameters()}\n",
    "        self.fisher = self._compute_fisher(dataloader, samples)\n",
    "\n",
    "    def _compute_fisher(self, dataloader, samples):\n",
    "        fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters()}\n",
    "        self.model.eval()\n",
    "        count = 0\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(self.device)\n",
    "            self.model.zero_grad()\n",
    "            feats, logits = self.model(x)\n",
    "            # prob = F.softmax(logits, dim=1)\n",
    "\n",
    "            # log_prob = F.log_softmax(logits, dim=1)[range(len(y)), y].mean()\n",
    "            # log_prob.backward()\n",
    "\n",
    "            log_probs = F.log_softmax(logits, dim=1)\n",
    "            # sum negative log‐likelihood over batch\n",
    "            loss_batch = -log_probs[range(len(y)), y].sum()\n",
    "            loss_batch.backward()\n",
    "            for n, p in self.model.named_parameters():\n",
    "                fisher[n] += p.grad.data.pow(2)\n",
    "            count += x.size(0)   # count by number of *samples*\n",
    "            if count >= samples:\n",
    "                break\n",
    "        return {n: f / count for n, f in fisher.items()}\n",
    "\n",
    "    def penalty(self, model, lambda_ewc):\n",
    "        loss = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            if n not in self.fisher: continue\n",
    "            f, p0 = self.fisher[n], self.params[n]\n",
    "            if p.shape == p0.shape:\n",
    "                loss += (f * (p - p0).pow(2)).sum()\n",
    "            else:\n",
    "                # assume this is the expanded fc.weight or fc.bias\n",
    "                # only penalize the first p0.shape[...] entries\n",
    "                if 'fc.weight' in n and p.dim()==2:\n",
    "                    loss += (f * (p[:p0.size(0)] - p0).pow(2)).sum()\n",
    "                elif 'fc.bias' in n and p.dim()==1:\n",
    "                    loss += (f * (p[:p0.size(0)] - p0).pow(2)).sum()\n",
    "        return (lambda_ewc / 2) * loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
