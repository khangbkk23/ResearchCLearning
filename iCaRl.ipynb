{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0373d5c",
   "metadata": {},
   "source": [
    "# ICaRL: The strategy of taking buffer for EWC in Continual Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e37ced9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Notebook last modified at: 2025-07-27 23:02:52\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchvision.models import resnet34 as torchvision_resnet34\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, random_split, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Checking status of GPU and time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Notebook last modified at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ab334",
   "metadata": {},
   "source": [
    "## 1. Replay Buffer with per-class exemplars capturing the distribution of each class\n",
    "- The buffer is used to store a fixed number of exemplars for each class.\n",
    "- The number of exemplars per class is determined by the `exemplars_per_class`\n",
    "  parameter.\n",
    "- The buffer is updated with new exemplars when the model encounters new classes.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17340147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_per_class=100):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer with a maximum number of exemplars per class.\n",
    "        \n",
    "        Args:\n",
    "            max_per_class (int): Maximum number of exemplars to store for each class.\n",
    "        \"\"\"\n",
    "        self.max_per_class = max_per_class\n",
    "        self.buffer = defaultdict(list)\n",
    "        \n",
    "    def add_examples(self, x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Add examples to the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            x_batch (torch.Tensor): Batch of input data.\n",
    "            y_batch (torch.Tensor): Corresponding labels for the input data.\n",
    "        \"\"\"\n",
    "        for x, y in zip(x_batch, y_batch):\n",
    "            cls = int(y.item())\n",
    "            lst = self.buffer[cls]\n",
    "            lst.append(x)\n",
    "            # FIFO replacement if the class buffer exceeds the limit\n",
    "            if len(lst) > self.max_per_class:\n",
    "                lst.pop(0)\n",
    "            self.buffer[cls] = lst\n",
    "    \n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        Get all data from the replay buffer as a TensorDataset.\n",
    "        \n",
    "        Returns:\n",
    "            TensorDataset: A dataset containing all exemplars in the buffer.\n",
    "        \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for cls, examples in self.buffer.item():\n",
    "            xs.append(torch.stack(examples)) # Collect all examples for the class\n",
    "            ys.append(torch.full((len(examples), 1), cls, dtype=torch.long))\n",
    "        if not xs:\n",
    "            return None, None\n",
    "        return torch.cat(xs, dim=0), torch.cat(ys, dim=0)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd6b51",
   "metadata": {},
   "source": [
    "## 2. iCaRL buffer with memory budget memory_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b33891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICaRLBuffer:\n",
    "    def __init__(self, memory_size):\n",
    "        \"\"\"\n",
    "        Initialize the iCaRL buffer with a memory budget.\n",
    "        \n",
    "        Args:\n",
    "            memory_size (int): Total memory budget for the buffer.\n",
    "        \"\"\"\n",
    "        self.memory_size = memory_size # maximum buffer size\n",
    "        self.exemplar_set = defaultdict(list) # class_id -> list of exemplars tensors\n",
    "        self.seen_classes = set() # keep track of seen classes\n",
    "    \n",
    "    def construct_exemplar_set(self, class_id, features, images, m):\n",
    "        \"\"\"\n",
    "        Construct the exemplar set for a given class.\n",
    "        \n",
    "        Args:\n",
    "            class_id (int): The class ID for which to construct the exemplar set.\n",
    "            features (torch.Tensor): Features of the images.\n",
    "            images (torch.Tensor): Corresponding images.\n",
    "            m (int): Number of exemplars to select for this class.\n",
    "        \"\"\"\n",
    "        features = F.normalize(features, dim=1) # normalize features\n",
    "        class_mean = F.normalize(class_mean.unsqueeze(0), dim=1) # normalize class mean\n",
    "        \n",
    "        selected, exemplar_features = [], []\n",
    "        used_indices = torch.zeros(features.size(0), dtype=torch.bool, device=features.device)\n",
    "        \n",
    "        for k in range(m):\n",
    "            if k == 0:\n",
    "                current_sum = 0\n",
    "            else:\n",
    "                current_sum = torch.stack(exemplar_features).sum(dim=0)\n",
    "            mu = class_mean.squeeze(0)\n",
    "            residual = mu * (k + 1) - current_sum\n",
    "            distances = (features @ residual).squeeze()\n",
    "            \n",
    "            # Mask already used indices\n",
    "            distances[used_indices] = float('-inf')\n",
    "            idx = torch.argmax(distances).item()\n",
    "            \n",
    "            selected.append(images[idx].cpu())\n",
    "            exemplar_features.append(features[idx].cpu())\n",
    "            used_indices[idx] = True\n",
    "            \n",
    "        self.exemplar_set[class_id] = selected\n",
    "        self.seen_classes.add(class_id)\n",
    "    \n",
    "    def reduce_exemplar_sets(self, m_per_class):\n",
    "        \"\"\"\n",
    "        Reduce the exemplar sets to fit within the memory budget.\n",
    "        \n",
    "        Args:\n",
    "            m_per_class (int): Number of exemplars to keep per class.\n",
    "        \"\"\"\n",
    "        for cls in self.seen_classes:\n",
    "            if len(self.exemplar_set[cls]) > m_per_class:\n",
    "                self.exemplar_set[cls] = self.exemplar_set[cls][:m_per_class]\n",
    "    \n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        Get all data from the iCaRL buffer as a TensorDataset.\n",
    "        \n",
    "        Returns:\n",
    "            TensorDataset: A dataset containing all exemplars in the buffer.\n",
    "        \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for cls, examples in self.exemplar_set.items():\n",
    "            if examples:\n",
    "                xs.append(torch.stack(examples))\n",
    "                ys.append(torch.full((len(examples), 1), cls, dtype=torch.long))\n",
    "        if not xs:\n",
    "            return None, None\n",
    "        return torch.cat(xs, dim=0), torch.cat(ys, dim=0)\n",
    "    \n",
    "    def get_all_data_for_task(self, class_list):\n",
    "        \"\"\"\n",
    "        Get all data for a specific task from the iCaRL buffer.\n",
    "        \n",
    "        Args:\n",
    "            class_list (list): List of class IDs for the task.\n",
    "        \n",
    "        Returns:\n",
    "            TensorDataset: A dataset containing all exemplars for the specified classes.\n",
    "        \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for cls in class_list:\n",
    "            exemplars = self.exemplar_sets.get(cls, [])\n",
    "            if len(exemplars) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Stack the list-of‐tensors → a single tensor of shape [num_exemplars_of_cls, ...]\n",
    "            xs.append(torch.stack(exemplars))\n",
    "            # Create a label‐tensor of shape [num_exemplars_of_cls] filled with “cls”\n",
    "            ys.append(torch.full((len(exemplars),), cls, dtype=torch.long))\n",
    "\n",
    "        if not xs:\n",
    "            return None, None\n",
    "\n",
    "        # Concatenate along the “batch” dimension\n",
    "        all_x = torch.cat(xs, dim=0)\n",
    "        all_y = torch.cat(ys, dim=0)\n",
    "        return all_x, all_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0255b",
   "metadata": {},
   "source": [
    "## 3. Small ResNet-34 model with 2 blocks\n",
    "- The model is a small ResNet-34 architecture with 2 blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf8cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNetSmall(nn.Module):\n",
    "    \"\"\"\n",
    "    Small ResNet-34 model with 2 blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = BasicBlock(16, 32, stride=2)\n",
    "        self.layer2 = BasicBlock(32, 64, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.feature_dim = 64\n",
    "        self.fc = nn.Linear(self.feature_dim, num_classes)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"\n",
    "            Extract features from the input tensor.\n",
    "            Params:\n",
    "                x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "            Returns:\n",
    "                torch.Tensor: Extracted features of shape (batch_size, feature_dim).\n",
    "        \"\"\"\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.avgpool(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass through the network.\n",
    "            Params:\n",
    "                x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "        \"\"\"\n",
    "        feats = self.extract_features(x)\n",
    "        logits = self.fc(feats)\n",
    "        return feats, logits\n",
    "\n",
    "    def expand_output(self, new_num_classes):\n",
    "        \"\"\"\n",
    "        Expand the output layer to accommodate new classes.\n",
    "        Args:\n",
    "            new_num_classes (int): The new number of classes for the output layer.\n",
    "        \"\"\"\n",
    "        old_fc = self.fc\n",
    "        new_fc = nn.Linear(self.feature_dim, new_num_classes)\n",
    "        with torch.no_grad():\n",
    "            # copy old parameters of FC layer to newly expanded model\n",
    "            new_fc.weight[:old_fc.out_features] = old_fc.weight\n",
    "            new_fc.bias[:old_fc.out_features] = old_fc.bias\n",
    "        self.fc = new_fc.to(old_fc.weight.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b811c",
   "metadata": {},
   "source": [
    "## 4. ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81df3f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1  # For BasicBlock, output channels = out_channels * 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 3)\n",
    "        self.layer2 = self._make_layer(128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 3, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.feature_dim = 512\n",
    "        self.fc = nn.Linear(self.feature_dim, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.extract_features(x)\n",
    "        logits = self.fc(feats)\n",
    "        return feats, logits\n",
    "\n",
    "    def expand_output(self, new_num_classes):\n",
    "        old_fc = self.fc\n",
    "        new_fc = nn.Linear(self.feature_dim, new_num_classes)\n",
    "        with torch.no_grad():\n",
    "            new_fc.weight[:old_fc.out_features] = old_fc.weight\n",
    "            new_fc.bias[:old_fc.out_features] = old_fc.bias\n",
    "        self.fc = new_fc.to(old_fc.weight.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b301b2",
   "metadata": {},
   "source": [
    "## 5. Elastic Weight Consolidation (EWC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcfed9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 3. Elastic Weight Consolidation (EWC) ====  \n",
    "class EWC:\n",
    "    def __init__(self, model, dataloader, device, samples=500):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.params = {n: p.clone().detach() for n, p in model.named_parameters()}\n",
    "        self.fisher = self._compute_fisher(dataloader, samples)\n",
    "\n",
    "    def _compute_fisher(self, dataloader, samples):\n",
    "        fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters()}\n",
    "        self.model.eval()\n",
    "        count = 0\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(self.device)\n",
    "            self.model.zero_grad()\n",
    "            feats, logits = self.model(x)\n",
    "            # prob = F.softmax(logits, dim=1)\n",
    "\n",
    "            # log_prob = F.log_softmax(logits, dim=1)[range(len(y)), y].mean()\n",
    "            # log_prob.backward()\n",
    "\n",
    "            log_probs = F.log_softmax(logits, dim=1)\n",
    "            # sum negative log‐likelihood over batch\n",
    "            loss_batch = -log_probs[range(len(y)), y].sum()\n",
    "            loss_batch.backward()\n",
    "            for n, p in self.model.named_parameters():\n",
    "                fisher[n] += p.grad.data.pow(2)\n",
    "            count += x.size(0)   # count by number of *samples*\n",
    "            if count >= samples:\n",
    "                break\n",
    "        return {n: f / count for n, f in fisher.items()}\n",
    "\n",
    "    def penalty(self, model, lambda_ewc):\n",
    "        loss = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            if n not in self.fisher: continue\n",
    "            f, p0 = self.fisher[n], self.params[n]\n",
    "            if p.shape == p0.shape:\n",
    "                loss += (f * (p - p0).pow(2)).sum()\n",
    "            else:\n",
    "                # assume this is the expanded fc.weight or fc.bias\n",
    "                # only penalize the first p0.shape[...] entries\n",
    "                if 'fc.weight' in n and p.dim()==2:\n",
    "                    loss += (f * (p[:p0.size(0)] - p0).pow(2)).sum()\n",
    "                elif 'fc.bias' in n and p.dim()==1:\n",
    "                    loss += (f * (p[:p0.size(0)] - p0).pow(2)).sum()\n",
    "        return (lambda_ewc / 2) * loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717fc3e2",
   "metadata": {},
   "source": [
    "## 6. Fine-tuning ResNet model and EWC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f4ec68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerTaskEWC:\n",
    "    \"\"\"\n",
    "    Collects multiple (params, fisher) snapshots—one per past task—\n",
    "    and, at training‐time, computes the sum of all EWC penalties.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, device, ewc_paths: list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - model (nn.Module):  The “current” model (whose parameter names\n",
    "                                must match those stored on disk).\n",
    "          - device:            CPU / CUDA device.\n",
    "          - ewc_paths:         List of file‐paths: ['ewc_task_1.pt', 'ewc_task_2.pt', ...].\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "\n",
    "        self.past_task_params = []  # list of dict: each dict maps name→tensor (θ^{*(k)})\n",
    "        self.past_task_fishers = [] # list of dict: each dict maps name→tensor (F^{(k)})\n",
    "\n",
    "        # Load all saved EWC files:\n",
    "        for path in ewc_paths:\n",
    "            data = torch.load(path, map_location='cpu')\n",
    "            # data['params'] and data['fisher'] are both dict(name→cpu_tensor)\n",
    "            # Move them to the correct device now:\n",
    "            params_k = {name: param.to(self.device) for name, param in data['params'].items()}\n",
    "            fisher_k = {name: fisher.to(self.device) for name, fisher in data['fisher'].items()}\n",
    "            self.past_task_params.append(params_k)\n",
    "            self.past_task_fishers.append(fisher_k)\n",
    "\n",
    "\n",
    "    def penalty(self, model, lambda_ewc):\n",
    "        \"\"\"\n",
    "        Loops over each past task k, then each parameter name,\n",
    "        and accumulates F^{(k)}_i * (θ_i - θ^{*(k)}_i)^2.\n",
    "\n",
    "        Returns:  (λ/2) * [sum over tasks & params of F (θ - θ*)^2]\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Iterate over each past‐task snapshot:\n",
    "        for params_k, fisher_k in zip(self.past_task_params, self.past_task_fishers):\n",
    "            for name, param in model.named_parameters():\n",
    "                # If this parameter existed when snapshot_k was taken:\n",
    "                if name not in fisher_k:\n",
    "                    continue\n",
    "\n",
    "                θ_star = params_k[name]      # θ^{*(k)}\n",
    "                Fk      = fisher_k[name]     # F^{(k)}\n",
    "\n",
    "                if param.shape == θ_star.shape:\n",
    "                    total_loss += (Fk * (param - θ_star).pow(2)).sum()\n",
    "                else:\n",
    "                    # If some layers were expanded (e.g. classifier head grew),\n",
    "                    # only penalize the “old” slice [0:θ_star.shape[...]].\n",
    "                    if 'fc.weight' in name and param.dim() == 2:\n",
    "                        total_loss += (Fk * (param[:θ_star.size(0)] - θ_star).pow(2)).sum()\n",
    "                    elif 'fc.bias' in name and param.dim() == 1:\n",
    "                        total_loss += (Fk * (param[:θ_star.size(0)] - θ_star).pow(2)).sum()\n",
    "                    # else: if other layers changed shape unexpectely, you may skip them.\n",
    "\n",
    "        # Multiply by λ/2:\n",
    "        return (lambda_ewc / 2) * total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec9143",
   "metadata": {},
   "source": [
    "# Mahalanobis-distance based Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e5cd0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 4. Mahalanobis-distance based Detector ====  \n",
    "class MahalanobisDetector:\n",
    "    def __init__(self):\n",
    "        self.class_means = {}\n",
    "        self.precision = None\n",
    "        self.tau = None\n",
    "\n",
    "    def fit(self, model, buffer, device, reg_cov=1e-5):\n",
    "        model.eval()\n",
    "        X, Y = buffer.get_all_data()\n",
    "        if X is None:\n",
    "            return\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        feats = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X), 256):\n",
    "                batch = X[i:i+256].float()\n",
    "                f, _ = model(batch)\n",
    "                feats.append(f)\n",
    "        feats = torch.cat(feats, dim=0)\n",
    "        for c in torch.unique(Y):\n",
    "            self.class_means[int(c.item())] = feats[Y==c].mean(dim=0)\n",
    "        centered = feats - torch.stack([self.class_means[int(y.item())] for y in Y])\n",
    "        cov = (centered.t() @ centered) / (len(Y)-1)\n",
    "        cov += reg_cov * torch.eye(cov.size(0)).to(device)\n",
    "        self.precision = torch.inverse(cov)\n",
    "\n",
    "    def score(self, model, x, device):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            f, _ = model(x)\n",
    "        dists = []\n",
    "        for mu in self.class_means.values():\n",
    "            diff = f - mu.unsqueeze(0)\n",
    "            dists.append(torch.sum(diff @ self.precision * diff, dim=1))\n",
    "        dists = torch.stack(dists, dim=1)\n",
    "        # print(f\"dists={dists}\")\n",
    "        return dists.min(dim=1)[0]\n",
    "\n",
    "    def detect(self, model, x, device):\n",
    "        return self.score(model, x, device) > self.tau\n",
    "\n",
    "    def set_threshold(self, model, buffer, device, false_positive_rate=0.2):\n",
    "        X_id, _ = buffer.get_all_data()\n",
    "        if X_id is None:\n",
    "            return\n",
    "        d_id = []\n",
    "        for i in range(0, len(X_id), 256):\n",
    "            d_id.append(self.score(model, X_id[i:i+256].to(device), device))\n",
    "        d_id = torch.cat(d_id)\n",
    "        self.tau = torch.quantile(d_id, 1 - false_positive_rate).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ebf60",
   "metadata": {},
   "source": [
    "# Training with detector gating, replay, EWC and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb020d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def train_and_plot(\n",
    "    train_loaders, test_loaders, ood_loader, device, args\n",
    "):\n",
    "    epochs_per_task = args.epochs\n",
    "    num_tasks = len(train_loaders) \n",
    "    total_epochs = epochs_per_task * args.num_tasks \n",
    "    history = {t: [np.nan] * total_epochs for t in range(num_tasks)}\n",
    "\n",
    "    # use pretrained model\n",
    "    if args.dataset not in ['mnist']:\n",
    "        model = ResNet34(num_classes=args.num_cls_per_task)\n",
    "        if args.use_pretrained:\n",
    "            pretrained = torchvision_resnet34(pretrained=True)\n",
    "            # Grab their state_dicts\n",
    "            pre_sd = pretrained.state_dict()\n",
    "            model_sd  = model.state_dict()\n",
    "            # Filter out weights that don’t match (e.g. fc.weight, fc.bias)\n",
    "            filtered_pre_sd = {\n",
    "                k: v\n",
    "                for k, v in pre_sd.items()\n",
    "                if k in model_sd and v.shape == model_sd[k].shape\n",
    "            }\n",
    "            # Overwrite matching keys in our model’s dict\n",
    "            model_sd.update(filtered_pre_sd)\n",
    "            # Load back into our custom model\n",
    "            model.load_state_dict(model_sd)\n",
    "    else:\n",
    "        model = ResNetSmall(num_classes=args.num_cls_per_task)\n",
    "    model = model.to(device)\n",
    "\n",
    "    log_file_path = os.path.join(args.savedir, 'output_log.txt')\n",
    "    plot_file_path = os.path.join(args.savedir, 'accuracy.png')\n",
    "    current_classes = 0\n",
    "    buffer = iCaRLBuffer(memory_size=args.memory_size)  # set e.g., 2000\n",
    "    detector = MahalanobisDetector()\n",
    "    ewc = None\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    #–– Prepare a cycling OOD iterator ––\n",
    "    ood_iter = iter(ood_loader)\n",
    "\n",
    "    detector.fit(model, buffer, device)\n",
    "    detector.set_threshold(model, buffer, device, args.fpr_rate)\n",
    "\n",
    "    for t, base_loader in enumerate(train_loaders):\n",
    "        # ───────────────────────────────────────────────────────────────────────────\n",
    "        # (A) Gate / filter new task’s data via Mahalanobis detector (if t > 0)\n",
    "        if t > 0:\n",
    "            model.expand_output(current_classes + args.num_cls_per_task)\n",
    "            current_classes += args.num_cls_per_task\n",
    "            optimizer = optim.SGD(model.parameters(), lr=args.lr,\n",
    "                                  momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    \n",
    "            indices = []\n",
    "            dataset = base_loader.dataset\n",
    "            for idx in range(len(dataset)):\n",
    "                x_img, y_img = dataset[idx]\n",
    "                x_tensor = x_img.unsqueeze(0).to(device)\n",
    "                if detector.detect(model, x_tensor, device):\n",
    "                    indices.append(idx)\n",
    "            with open(log_file_path, \"a\") as file:\n",
    "                print(f\"New defect found for task {t}: {len(indices)}/{len(dataset)}\", file=file)\n",
    "            inbound = Subset(dataset, indices)\n",
    "            new_task_dataset = inbound\n",
    "        else:\n",
    "            new_task_dataset = base_loader.dataset\n",
    "    \n",
    "        # ───────────────────────────────────────────────────────────────────────────\n",
    "        # (B) Wrap new_task_dataset so that its labels are Tensors, not plain ints.\n",
    "        new_task_dataset = LabelTransformDataset(new_task_dataset)\n",
    "    \n",
    "        #  (C) Build “combined” = {new_task_dataset} ∪ {all exemplars from buffer}\n",
    "        buf_x, buf_y = buffer.get_all_data()\n",
    "        if buf_x is not None:\n",
    "            # Exemplar labels are already torch.LongTensor, since buf_y is a tensor.\n",
    "            exemplar_dataset = TensorDataset(buf_x, buf_y)\n",
    "            combined_dataset = ConcatDataset([new_task_dataset, exemplar_dataset])\n",
    "        else:\n",
    "            # If buffer is empty, just train on new_task_dataset\n",
    "            combined_dataset = new_task_dataset\n",
    "    \n",
    "        combined_loader = DataLoader(\n",
    "            combined_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # -------------------------------------\n",
    "        # Build a list of previous ewc paths\n",
    "        # -------------------------------------\n",
    "        prev_ewc_paths = [f'{args.savedir}/ewc_task_{k}.pt' for k in range(0, t)]\n",
    "        print(f\"len(prev_ewc_paths)={len(prev_ewc_paths)}\")\n",
    "        if len(prev_ewc_paths) > 0:\n",
    "            per_task_ewc = PerTaskEWC(model, device, prev_ewc_paths)\n",
    "        else:\n",
    "            per_task_ewc = None\n",
    "    \n",
    "        # ───────────────────────────────────────────────────────────────────────────\n",
    "        # (D) Now train for all epochs on combined_loader\n",
    "        for e in range(epochs_per_task):\n",
    "            global_epoch = t * epochs_per_task + e\n",
    "            print(f\"Global epoch: {global_epoch}\")\n",
    "            model.train()\n",
    "    \n",
    "            for idx, (x_batch, y_batch) in enumerate(combined_loader):\n",
    "                x = x_batch.to(device)\n",
    "                y = y_batch.to(device)\n",
    "    \n",
    "                feats, logits = model(x)\n",
    "                loss_task = F.cross_entropy(logits, y)\n",
    "    \n",
    "                if per_task_ewc is not None:\n",
    "                    loss_penalty = per_task_ewc.penalty(model, args.lambda_ewc)\n",
    "                    loss = loss_task + loss_penalty\n",
    "                else:\n",
    "                    loss = loss_task\n",
    "\n",
    "                if t > 0:\n",
    "                    try:\n",
    "                        x_ood, _ = next(ood_iter)\n",
    "                    except StopIteration:\n",
    "                        ood_iter = iter(ood_loader)\n",
    "                        x_ood, _ = next(ood_iter)\n",
    "                    x_ood = x_ood.to(device)\n",
    "    \n",
    "                    scores_in  = detector.score(model, x, device)\n",
    "                    scores_out = detector.score(model, x_ood, device)\n",
    "                    margin_in  = torch.clamp(scores_in  - detector.tau, min=0.0).mean()\n",
    "                    margin_out = torch.clamp(detector.tau - scores_out, min=0.0).mean()\n",
    "                    loss = loss + args.lambda_ood * (margin_in + margin_out)\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "            # Refit detector & threshold\n",
    "            detector.fit(model, buffer, device)\n",
    "            detector.set_threshold(model, buffer, device, args.fpr_rate)\n",
    "            # Evaluate all seen tasks\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for tt in range(t+1):\n",
    "                    correct, total = 0, 0\n",
    "                    for x_test, y_test in test_loaders[tt]:\n",
    "                        x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "                        _, logits = model(x_test)\n",
    "                        preds = logits.argmax(dim=1)\n",
    "                        correct += (preds == y_test).sum().item()\n",
    "                        total += y_test.size(0)\n",
    "                    history[tt][global_epoch] = correct / total\n",
    "        # Save model after task t\n",
    "        torch.save(model.state_dict(), f'{args.savedir}/model_task_{t}.pt')\n",
    "        print(f\"Model saved for task {t} as '{args.savedir}/model_task_{t}.pt'\")\n",
    "       \n",
    "       # ───────────────────────────────────────────────────────────────────────────\n",
    "        #  (1) Compute “cls_start” and “cls_end” for task t\n",
    "        #      Example: if num_cls_per_task = 2:\n",
    "        #         task 0  → cls_start = 0*2 = 0, cls_end = 2 → classes [0,1]\n",
    "        #         task 1  → cls_start = 1*2 = 2, cls_end = 4 → classes [2,3]\n",
    "        #         etc.\n",
    "        num_per_task = args.num_cls_per_task\n",
    "        cls_start = t * num_per_task\n",
    "        cls_end   = cls_start + num_per_task  # (exclusive upper bound)\n",
    "    \n",
    "        # ───────────────────────────────────────────────────────────────────────────\n",
    "        #  (2) Build “features_all” & “labels_all” from the UNFILTERED task‐t data\n",
    "        #      i.e. from base_loader.dataset, which has *all* images of these classes\n",
    "        features_all, labels_all = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # We iterate over the *entire* base_loader.dataset to get its features+labels\n",
    "            for x_batch, y_batch in DataLoader(base_loader.dataset, batch_size=64, shuffle=False):\n",
    "                x_batch = x_batch.to(device)\n",
    "                feats, _ = model(x_batch)        # feats.shape = [batch_size, feat_dim]\n",
    "                features_all.append(feats.cpu()) # store on CPU so we can index easily\n",
    "                labels_all.append(y_batch)       # y_batch is on CPU by default\n",
    "    \n",
    "        features_all = torch.cat(features_all, dim=0)  # shape: [N_task_samples, feat_dim]\n",
    "        labels_all   = torch.cat(labels_all, dim=0)    # shape: [N_task_samples]\n",
    "    \n",
    "        # ───────────────────────────────────────────────────────────────────────────\n",
    "        #  (3) Compute how many exemplars PER class we should keep\n",
    "        #      Because the buffer must hold at most memory_size total exemplars,\n",
    "        #      and we now have seen (t+1)*num_per_task distinct classes.\n",
    "        num_seen_classes = (t + 1) * num_per_task\n",
    "        m_per_class = buffer.memory_size // num_seen_classes\n",
    "    \n",
    "        # ───────────────────────────────────────────────────────────────────────────\n",
    "        #  (4) For each *new* class cls in [cls_start, cls_end), pick m_per_class exemplars\n",
    "        for cls in range(cls_start, cls_end):\n",
    "            # Boolean mask: selects features_all[i] whose label == cls\n",
    "            cls_mask = (labels_all == cls)            # shape: [N_task_samples], True for indices of class `cls`\n",
    "            idxs = torch.nonzero(cls_mask, as_tuple=True)[0].tolist()\n",
    "            # Now `idxs` indexes INTO base_loader.dataset. Since base_loader.dataset\n",
    "            # has ALL samples of class `cls`, idxs can never be empty here (unless you truly had 0 samples)\n",
    "            if len(idxs) == 0:\n",
    "                # (This check is purely defensive; normally base_loader.dataset must have at least 1 sample per class.)\n",
    "                print(f\"[Warning] No samples found for class {cls} in base_loader.dataset!\")\n",
    "                continue\n",
    "    \n",
    "            # Build a list of raw images (tensors) for these indices\n",
    "            imgs_list = [base_loader.dataset[i][0] for i in idxs]\n",
    "            # Stack them to a single tensor of shape [N_cls_samples, C, H, W]\n",
    "            imgs_cls = torch.stack(imgs_list, dim=0)\n",
    "    \n",
    "            # Also extract the corresponding feature vectors from features_all\n",
    "            feats_cls = features_all[cls_mask]       # shape [N_cls_samples, feat_dim]\n",
    "    \n",
    "            # Now finally construct the exemplar set for class `cls` via herding\n",
    "            buffer.construct_exemplar_set(cls, feats_cls, imgs_cls, m=m_per_class)\n",
    "        # (4b) After we add new exemplars, we must *shrink* older classes’ exemplars\n",
    "        buffer.reduce_exemplar_sets(m_per_class)\n",
    "    \n",
    "        # Update how many classes we have processed so far\n",
    "        current_classes = num_seen_classes\n",
    "    \n",
    "        # ───────────────────────────────────────────────────────────────────────────\n",
    "        #  (5) Now that `buffer` is non‐empty, rebuild EWC & refit the Mahalanobis detector\n",
    "        # all_x, all_y = buffer.get_all_data()\n",
    "        all_x, all_y = buffer.get_all_data_for_task([i for i in range(t * num_per_task, (t+1) * num_per_task)])  # or raw Task t data\n",
    "        if all_x is not None:\n",
    "            # (5a) Recompute EWC Fisher/info on the *entire* exemplar set\n",
    "            dataset_all = TensorDataset(all_x.float(), all_y)\n",
    "            ewc = EWC(model, DataLoader(dataset_all, batch_size=32, shuffle=True), device)\n",
    "    \n",
    "            # (5b) Refit the Mahalanobis detector on all exemplars\n",
    "            detector.fit(model, buffer, device)\n",
    "            detector.set_threshold(model, buffer, device, args.fpr_rate)\n",
    "    \n",
    "            # (5c) (Optional) Save EWC state to disk\n",
    "            ewc_data = {\n",
    "                'params': {k: v.cpu() for k, v in ewc.params.items()},\n",
    "                'fisher': {k: v.cpu() for k, v in ewc.fisher.items()}\n",
    "            }\n",
    "            torch.save(ewc_data, f'{args.savedir}/ewc_task_{t}.pt')\n",
    "            print(f\"EWC saved for task {t} as '{args.savedir}/ewc_task_{t}.pt'\")\n",
    "\n",
    "    sum_acc = 0\n",
    "    for i in range(len(train_loaders)):\n",
    "        sum_acc += history[i][global_epoch] * 100\n",
    "        with open(log_file_path, \"a\") as file:\n",
    "            print(f\"Final test accuracy for task {i}: {history[i][global_epoch] * 100}\", file=file)\n",
    "    with open(log_file_path, \"a\") as file:\n",
    "        print(f\"Average accuracy across {len(train_loaders)} tasks: {sum_acc / len(train_loaders)}\", file=file)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8,6))\n",
    "    labels = [f'Task {t}' for t in range(num_tasks)]\n",
    "    for t in range(num_tasks):\n",
    "        arr = np.array(history[t])\n",
    "        idxs = ~np.isnan(arr)\n",
    "        plt.plot(np.arange(total_epochs)[idxs], arr[idxs], label=labels[t])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0.2, 1.02)\n",
    "    plt.savefig(plot_file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8516f633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running experiment 1/1: {'lambda_ewc': 0.5, 'lambda_ood': 1.0, 'fpr_rate': 0.1, 'dataset': 'cifar10'} ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:28<00:00, 5.90MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dikhang_hcmut/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dikhang_hcmut/miniconda3/envs/pytorch_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /home/dikhang_hcmut/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83.3M/83.3M [00:02<00:00, 42.2MB/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'iCaRLBuffer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 139\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[43mtrain_and_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mood_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtrain_and_plot\u001b[39m\u001b[34m(train_loaders, test_loaders, ood_loader, device, args)\u001b[39m\n\u001b[32m     33\u001b[39m plot_file_path = os.path.join(args.savedir, \u001b[33m'\u001b[39m\u001b[33maccuracy.png\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     34\u001b[39m current_classes = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m buffer = \u001b[43miCaRLBuffer\u001b[49m(memory_size=args.memory_size)  \u001b[38;5;66;03m# set e.g., 2000\u001b[39;00m\n\u001b[32m     36\u001b[39m detector = MahalanobisDetector()\n\u001b[32m     37\u001b[39m ewc = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'iCaRLBuffer' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset, TensorDataset, Subset\n",
    "class LabelTransformDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wrap any dataset whose __getitem__(idx) returns (img_tensor, int_label)\n",
    "    and turn int_label → LongTensor(label).\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset):\n",
    "        self.dataset = base_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        return x, torch.tensor(y, dtype=torch.long)\n",
    "from torchvision.datasets import CIFAR10, STL10, MNIST, ImageFolder\n",
    "import itertools\n",
    "from types import SimpleNamespace\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Define your hyperparameter search space\n",
    "hyperparams = {\n",
    "    # 'lr': [0.001, 0.003],\n",
    "    'lambda_ewc': [0.5],\n",
    "    'lambda_ood': [1.0],\n",
    "    # 'batch_size': [128],\n",
    "    'fpr_rate': [0.1],\n",
    "    # 'max_per_class': [100],\n",
    "    'dataset': ['cifar10']\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "keys, values = zip(*hyperparams.items())\n",
    "experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "\n",
    "# Loop through each hyperparameter combination\n",
    "for i, exp in enumerate(experiments):\n",
    "    print(f\"\\n=== Running experiment {i+1}/{len(experiments)}: {exp} ===\")\n",
    "    \n",
    "    args = SimpleNamespace(\n",
    "        use_pretrained=True,\n",
    "        num_tasks=4,\n",
    "        num_cls_per_task=2,\n",
    "        lambda_ewc=exp['lambda_ewc'],\n",
    "        lambda_ood=exp['lambda_ood'],\n",
    "        fpr_rate=exp['fpr_rate'],\n",
    "        max_per_class=50,\n",
    "        lr=0.003,\n",
    "        momentum=0.9,\n",
    "        weight_decay=1e-4,\n",
    "        epochs=100,\n",
    "        batch_size=128,\n",
    "        dataset=exp['dataset'],\n",
    "        datadir='',\n",
    "        num_ood_cls=2,\n",
    "        memory_size=1600\n",
    "    )\n",
    "    tasks = [list(range(i,i+args.num_cls_per_task)) for i in range(0,args.num_cls_per_task*args.num_tasks,args.num_cls_per_task)]\n",
    "    if args.dataset != 'mnist':\n",
    "        train_transform = transforms.Compose([\n",
    "            # transforms.Resize(size=(224,224)),\n",
    "            # transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),\n",
    "            transforms.RandomHorizontalFlip(),              \n",
    "            transforms.RandomApply([\n",
    "                    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1)\n",
    "                ], p=0.8),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    dataset_path = os.path.join(os.getcwd(), args.dataset)\n",
    "    \n",
    "    # Check if the subdirectory exists, and create it if it doesn't\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.mkdir(dataset_path)\n",
    "    \n",
    "    args.savedir = os.path.join(dataset_path, f'{args.num_tasks}task_{args.num_cls_per_task}pertask_ewc{args.lambda_ewc}_ood{args.lambda_ood}_fpr{args.fpr_rate}_lr{args.lr}_{args.max_per_class}percls_{args.epochs}epochs_batch{args.batch_size}')\n",
    "    if not os.path.exists(args.savedir):\n",
    "        os.mkdir(args.savedir)\n",
    "    \n",
    "    if args.dataset == 'cifar10':\n",
    "        train_set = CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "        test_set  = CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "    elif args.dataset == 'mnist':\n",
    "        train_set = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        test_set = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    elif args.dataset == 'leather':\n",
    "        train_set = ImageFolder('/kaggle/input/leather-defect-classification/Leather Defect Classification/train', transform=train_transform)\n",
    "        test_set = ImageFolder('/kaggle/input/leather-defect-classification/Leather Defect Classification/valid', transform=test_transform)\n",
    "    elif args.dataset == 'texture':\n",
    "        train_set = ImageFolder('/kaggle/input/texture-classification/Texture/train', transform=train_transform)\n",
    "        test_set = ImageFolder('/kaggle/input/texture-classification/Texture/valid', transform=test_transform)\n",
    "    elif args.dataset == 'neu':\n",
    "        train_set = ImageFolder('/kaggle/input/neu-surface-defect-database/NEU-DET/train/images', transform=train_transform)\n",
    "        test_set = ImageFolder('/kaggle/input/neu-surface-defect-database/NEU-DET/validation/images', transform=test_transform)\n",
    "    elif args.dataset == 'gc10':\n",
    "        train_set = ImageFolder('/kaggle/input/gc10-det/GC10-DET/train', transform=train_transform)\n",
    "        test_set = ImageFolder('/kaggle/input/gc10-det/GC10-DET/valid', transform=test_transform)\n",
    "    elif args.dataset == 'ncat12':\n",
    "        train_set = ImageFolder('/kaggle/input/ncat12-det/NCAT12-DET/train', transform=train_transform)\n",
    "        test_set = ImageFolder('/kaggle/input/ncat12-det/NCAT12-DET/valid', transform=test_transform)\n",
    "    else:\n",
    "        train_set = ImageFolder(args.datadir + '/train', transform=transform)\n",
    "        test_set = ImageFolder(args.datadir + '/valid', transform=transform)\n",
    "    \n",
    "    def make_loader(dataset, task, batch_size=64, shuffle=True):\n",
    "        indices = [i for i, (_, y) in enumerate(dataset) if y in task]\n",
    "        subset = Subset(dataset, indices)\n",
    "        return DataLoader(subset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    class_indices = list(range(len(train_set.classes)))\n",
    "    train_loaders = [make_loader(train_set, task, batch_size=args.batch_size) for task in tasks]\n",
    "    test_loaders  = [make_loader(test_set, task, batch_size=args.batch_size)  for task in tasks]\n",
    "    num_ood_cls = args.num_ood_cls if args.num_ood_cls is not None else len(train_set.classes) - args.num_tasks * args.num_cls_per_task\n",
    "    ood_classes = class_indices[-num_ood_cls:]\n",
    "    ood_loader = make_loader(train_set, ood_classes, shuffle=False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"device: {device}\")\n",
    "    # Run training\n",
    "    train_and_plot(train_loaders, test_loaders, ood_loader, device, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
